{
  "llama4scout": {
    "stage3_only": {
      "precision": 0.511520737327189,
      "recall": 0.7302631578947368,
      "f1": 0.6016260162601627,
      "total_tp": 111,
      "total_fp": 106,
      "total_fn": 41,
      "total_predictions": 217,
      "total_gt_pages": 152
    },
    "stage1_3": {
      "precision": 0.5360824742268041,
      "recall": 0.6842105263157895,
      "f1": 0.6011560693641619,
      "total_tp": 104,
      "total_fp": 90,
      "total_fn": 48,
      "total_predictions": 194,
      "total_gt_pages": 152
    },
    "stage2_3": {
      "precision": 0.7202797202797203,
      "recall": 0.6776315789473685,
      "f1": 0.6983050847457628,
      "total_tp": 103,
      "total_fp": 40,
      "total_fn": 49,
      "total_predictions": 143,
      "total_gt_pages": 152
    },
    "full_pipeline": {
      "precision": 0.6666666666666666,
      "recall": 0.6447368421052632,
      "f1": 0.6555183946488294,
      "total_tp": 98,
      "total_fp": 49,
      "total_fn": 54,
      "total_predictions": 147,
      "total_gt_pages": 152
    }
  },
  "llama3.1-8b": {
    "stage3_only": {
      "precision": 0.30654761904761907,
      "recall": 0.6776315789473685,
      "f1": 0.4221311475409837,
      "total_tp": 103,
      "total_fp": 233,
      "total_fn": 49,
      "total_predictions": 336,
      "total_gt_pages": 152
    },
    "stage1_3": {
      "precision": 0.33226837060702874,
      "recall": 0.6842105263157895,
      "f1": 0.44731182795698926,
      "total_tp": 104,
      "total_fp": 209,
      "total_fn": 48,
      "total_predictions": 313,
      "total_gt_pages": 152
    },
    "stage2_3": {
      "precision": 0.44660194174757284,
      "recall": 0.6052631578947368,
      "f1": 0.5139664804469274,
      "total_tp": 92,
      "total_fp": 114,
      "total_fn": 60,
      "total_predictions": 206,
      "total_gt_pages": 152
    },
    "full_pipeline": {
      "precision": 0.4811320754716981,
      "recall": 0.6710526315789473,
      "f1": 0.5604395604395604,
      "total_tp": 102,
      "total_fp": 110,
      "total_fn": 50,
      "total_predictions": 212,
      "total_gt_pages": 152
    }
  },
  "llama3.2-3b": {
    "stage3_only": {
      "precision": 0.35294117647058826,
      "recall": 0.11842105263157894,
      "f1": 0.17733990147783252,
      "total_tp": 18,
      "total_fp": 33,
      "total_fn": 134,
      "total_predictions": 51,
      "total_gt_pages": 152
    },
    "stage1_3": {
      "precision": 0.35714285714285715,
      "recall": 0.09868421052631579,
      "f1": 0.15463917525773196,
      "total_tp": 15,
      "total_fp": 27,
      "total_fn": 137,
      "total_predictions": 42,
      "total_gt_pages": 152
    },
    "stage2_3": {
      "precision": 0.6666666666666666,
      "recall": 0.14473684210526316,
      "f1": 0.23783783783783782,
      "total_tp": 22,
      "total_fp": 11,
      "total_fn": 130,
      "total_predictions": 33,
      "total_gt_pages": 152
    },
    "full_pipeline": {
      "precision": 0.5588235294117647,
      "recall": 0.125,
      "f1": 0.20430107526881722,
      "total_tp": 19,
      "total_fp": 15,
      "total_fn": 133,
      "total_predictions": 34,
      "total_gt_pages": 152
    }
  },
  "llama3.3": {
    "stage3_only": {
      "precision": 0.3586626139817629,
      "recall": 0.7763157894736842,
      "f1": 0.49064449064449067,
      "total_tp": 118,
      "total_fp": 211,
      "total_fn": 34,
      "total_predictions": 329,
      "total_gt_pages": 152
    },
    "stage1_3": {
      "precision": 0.3538011695906433,
      "recall": 0.7960526315789473,
      "f1": 0.4898785425101215,
      "total_tp": 121,
      "total_fp": 221,
      "total_fn": 31,
      "total_predictions": 342,
      "total_gt_pages": 152
    },
    "stage2_3": {
      "precision": 0.5135135135135135,
      "recall": 0.75,
      "f1": 0.6096256684491977,
      "total_tp": 114,
      "total_fp": 108,
      "total_fn": 38,
      "total_predictions": 222,
      "total_gt_pages": 152
    },
    "full_pipeline": {
      "precision": 0.519650655021834,
      "recall": 0.7828947368421053,
      "f1": 0.6246719160104987,
      "total_tp": 119,
      "total_fp": 110,
      "total_fn": 33,
      "total_predictions": 229,
      "total_gt_pages": 152
    }
  },
  "mistral-small": {
    "stage3_only": {
      "precision": 0.5130890052356021,
      "recall": 0.6447368421052632,
      "f1": 0.5714285714285714,
      "total_tp": 98,
      "total_fp": 93,
      "total_fn": 54,
      "total_predictions": 191,
      "total_gt_pages": 152
    },
    "stage1_3": {
      "precision": 0.46875,
      "recall": 0.6907894736842105,
      "f1": 0.5585106382978723,
      "total_tp": 105,
      "total_fp": 119,
      "total_fn": 47,
      "total_predictions": 224,
      "total_gt_pages": 152
    },
    "stage2_3": {
      "precision": 0.7301587301587301,
      "recall": 0.6052631578947368,
      "f1": 0.6618705035971223,
      "total_tp": 92,
      "total_fp": 34,
      "total_fn": 60,
      "total_predictions": 126,
      "total_gt_pages": 152
    },
    "full_pipeline": {
      "precision": 0.7152777777777778,
      "recall": 0.6776315789473685,
      "f1": 0.695945945945946,
      "total_tp": 103,
      "total_fp": 41,
      "total_fn": 49,
      "total_predictions": 144,
      "total_gt_pages": 152
    }
  }
}